<!DOCTYPE html>
<html>

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Multi-region logging architecture with Logstash, Kibana, and ElasticSearch
    
  </title>

  <script type="application/javascript" src="http://code.jquery.com/jquery-1.10.0.min.js"></script>

  <!-- syntax highlighting CSS -->
  <link rel="stylesheet" href="/css/syntax.css">
  <link rel="stylesheet" href="/css/poole.css">
  <link rel="stylesheet" href="/css/hyde.css">
  <link rel="stylesheet" href="/css/custom.css">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">
  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-49052230-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
</head>

  <body class="theme-base-09">
    <div class="container content">
      <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1 class="sidebar-myname">Samuel Toriel</h1>
      <p class="lead">A blog about Software and Operations</p>
    </div>

    <ul class="sidebar-nav">
      <br />
      <li class="sidebar-nav-item">
        <a href="/">Home</a>
      </li>
      <li class="sidebar-nav-item"><a href="/all_posts.html">All posts</a></li>
      <li class="sidebar-nav-item"><a href="/about.html">About</a></li>
      <li class="sidebar-nav-item"><a class="rs-nav-link" href="https://twitter.com/rilt" target="_new">@rilt</a></li>
      <li class="sidebar-nav-item" target="_new"><a href="https://github.com/riltsken">riltsken on github</a></li>
      <li class="sidebar-nav-item"><a href="/atom.xml">Atom Feed</a></li>
    </ul>
  </div>
</div>

      <!-- Main Content -->
      <script type="text/javascript">
window.twttr=(function(d,s,id){var t,js,fjs=d.getElementsByTagName(s)[0];if(d.getElementById(id)){return}js=d.createElement(s);js.id=id;js.src="https://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);return window.twttr||(t={_e:[],ready:function(f){t._e.push(f)}})}(document,"script","twitter-wjs"));
</script>

<div class="post">
  <h1 class="post-title">Multi-region logging architecture with Logstash, Kibana, and ElasticSearch</h1>
  <span class="post-date">25 Mar 2014</span>
  <a class="twitter-share-button" href="https://twitter.com/share?size=large">Tweet</a>
  <h2>Background</h2>

<p>On my team right now we are using rsyslog with <a href="http://graylog2.org/">Graylog2</a> and <a href="http://www.elasticsearch.org/">ElasticSearch</a> to handle our logging infrastucture. The current setup is not ideal as we are distributed multi-region for our application in 3 datacenters (ORD, DFW, SYD) and each one has it&#39;s own cluster setup to use Graylog2 and ElasticSearch. This means if someone wanted to search through logs you would have to pick that specific region&#39;s Graylog2 instance. The original reason for this setup was that we had our logging infrastructure setup before multi-region was in place and we had to make a decision about how much time we wanted to spend setting it up. We chose for the quickest option as we had other product work that needed to get done before improving our logging infrastructure. This has proved to be a costly choice for us. Our current system has degraded to the point where we barely use our Graylog2 interface anymore. There are several reasons for this. One is that it is frustrating to switch between the multiple region interfaces and setup the same filters for each one. Another is that the version of Graylog2 + ElasticSearch we are working with are struggling to keep up with the amount of logs we have. It has gotten to the point where even simple queries executed on Graylog2 cause alerts to fire on our ElasticSearch cluster requiring action from us to help restore it.</p>

<p>Our backlog has some stories in place to remedy this situation, but are not on our radar for another few months. We recently had a hackweek and decided to experiment with some ideas and technology on what we want to use. Most of these ideas come from another team at Rackspace working on <a href="http://www.rackspace.com/cloud/auto-scale/">Autoscale</a>. Our idea is slightly modified, but the same general concept. None of this is currently implemented in a production like environment and most of it was setup in a test environment to play around with during our hackweek. The technologies in play here are <a href="http://logstash.net/">Logstash</a>, <a href="http://www.elasticsearch.org/overview/kibana/">Kibana</a>, and ElasticSearch.</p>

<p>I have to explain our current architecture a little bit first to setup why we would use the solution proposed first. The Cloud Control Panel at Rackspace is hosted in three different datacenters, ORD (US and Europe), DFW (backup), and SYD (Oceanic). All of our US and European traffic goes to ORD, while our Oceanic traffic goes to SYD. DFW is left as a warm backup that is ready in case any issue happens in the other two DC&#39;s. What we didn&#39;t want to do was make the same mistake as before with our logging and have multiple regional interfaces to access our logs. This meant collecting all of our logs and putting it into one datacenter for searching and querying. What that required was having each datacenter ship their logs to the collector which then puts these logs into ElasticSearch. There exists a node in each datacenter, called the broker, which then ships to the collector the logs for that datacenter. So let&#39;s go over this one more time. There is one collector node, one broker node per region shipping to the collector, and all nodes in the same datacenter ship nodes to their specified broker. We can then browse logs through our collector which will be running Kibana.</p>

<p><img src="http://d1dj72mlaoziwu.cloudfront.net/multi-region%20logging%20architecture.png" alt="Full picture of proposed multi-region logging infrastructure"></p>

<h2>Hackweek</h2>

<p>For the hackweek we broke up different portions of the infrastructure to different team members. I tackled setting up the broker node for a region and the collector. Having all nodes in the specified datacenter ship logs to the broker over a private network, which then in turn sent its logs to the collector on a public network over an encrypted channel. Our team already uses logstash for all of our nodes to send metrics to statsd, so most of the initial boostrapping of getting the logstash agent installed and running was already handled. We use Chef and Berkshelf to manage our infrastructure, which means we are using the <a href="https://github.com/lusis/chef-logstash">logstash cookbook</a> at version 0.7.6 at the time of this writing. Earlier versions of the cookbook had all the configuration rules for logstash written as node attributes which we put at the role level. As this method of creating rules was deprecated I moved them into configuration files that sit in the logstash conf.d directory. Logstash reads these config files in order, I found <a href="https://groups.google.com/forum/#!topic/logstash-users/eNYmpFueHtM">a convention I liked here</a> about numbering each config file which I decided to follow.</p>

<p>To get all nodes in a specific datacenter to send their logs to the broker I decided to use tags to handle forwarding the logs I wanted. An example of what this would look like via config files would be the following</p>
<div class="highlight"><pre><code class="language-erb" data-lang="erb">00_input_base.conf

input {
  file {
    'path' =&gt; '/var/log/rackspace-monitoring-agent.log',
    'type' =&gt; 'rackspace-monitoring-agent'
  }
}
</code></pre></div><div class="highlight"><pre><code class="language-erb" data-lang="erb">01_input_apache.conf

input {
  file {
    'path' =&gt; '/var/log/apache2/error.log'
    'type' =&gt; 'apache-error'
    'tags' =&gt; ['broker']
  }
  file {
    'path' =&gt; '/var/log/apache2/access.log'
    'type' =&gt; 'apache-access'
    'tags' =&gt; ['broker']
  }
}
</code></pre></div><div class="highlight"><pre><code class="language-erb" data-lang="erb">90_forward_to_broker.conf

output {
  if 'broker' in [tags] {
    redis {
      'host' =&gt; '192.168.9.1'
      'data_type' =&gt; 'list'
      'key' =&gt; 'logstash'
    }
  }
}
</code></pre></div>
<p><img src="http://d1dj72mlaoziwu.cloudfront.net/multi-region%20logging%20-%20web%20to%20broker.png" alt="Specific datacenter logging from application nodes to broker"></p>

<p>Basically what logstash would do is take all new events from the apache error and access logs, tag them with &#39;broker&#39;, and when logstash checks its outputs any events tagged with &#39;broker&#39; would get sent to our broker node. This is useful for us since we use logstash for processing and forwarding metrics to statsd from our logs as well. At this point we now have nodes in our environment forwarding to their local broker node in their datacenter.</p>

<p>The next step is to forward our logs to the collector. To do this we create a different set of conf.d files for our logstash agent to run on the broker node. For this portion we were originally going to use stunnel to create an encrypted channel for the logs to be sent over to the collector, however, as I was reading about the different inputs and outputs supported by logstash I stumbled on <a href="https://github.com/elasticsearch/logstash-forwarder">lumberjack</a>. Now, I actually had quite a few issues with understanding how lumberjack should be used in the context of logstash. What I see now is that it can either <strong>REPLACE</strong> the logstash agent as a log handler and forwarder or logstash can <strong>CREATE</strong> a lumberjack instance on the fly to send events as output or read events forwarded by lumberjack. In the first case, you would actually have to compile and build the lumberjack project and run the agent as a service. In the second case the logstash agent handles all of that and one should just use the inputs and outputs as normal.</p>
<div class="highlight"><pre><code class="language-erb" data-lang="erb">91_forward_to_collector.conf

input {
  redis {
    'host' =&gt; '<span class="cp">&lt;%=</span> <span class="n">node</span><span class="p">[</span><span class="s2">"logstash"</span><span class="p">][</span><span class="s2">"broker_ip"</span><span class="p">]</span> <span class="cp">%&gt;</span>'
    'data_type' =&gt; 'list'
    'key' =&gt; 'logstash'
  }
}

output {
  lumberjack {
    'hosts' =&gt; ['<span class="cp">&lt;%=</span> <span class="n">node</span><span class="p">[</span><span class="s2">"logstash"</span><span class="p">][</span><span class="s2">"collector_ip"</span><span class="p">]</span> <span class="cp">%&gt;</span>']
    'port' =&gt; <span class="cp">&lt;%=</span> <span class="n">node</span><span class="p">[</span><span class="s2">"logstash"</span><span class="p">][</span><span class="s2">"forwarder"</span><span class="p">][</span><span class="s2">"port"</span><span class="p">]</span> <span class="cp">%&gt;</span>
    'ssl_certificate' =&gt; '<span class="cp">&lt;%=</span> <span class="n">node</span><span class="p">[</span><span class="s2">"selfsigned_ssl"</span><span class="p">][</span><span class="s2">"directory"</span><span class="p">]</span> <span class="cp">%&gt;&lt;%=</span> <span class="n">node</span><span class="p">[</span><span class="s2">"selfsigned_ssl"</span><span class="p">][</span><span class="s2">"ssl_cert"</span><span class="p">]</span> <span class="cp">%&gt;</span>'
  }
}
</code></pre></div><div class="highlight"><pre><code class="language-erb" data-lang="erb">92_lumberjack_collector.conf

input {
  lumberjack {
    'host' =&gt; '127.0.0.1'
    'port' =&gt; <span class="cp">&lt;%=</span> <span class="n">node</span><span class="p">[</span><span class="s2">"logstash"</span><span class="p">][</span><span class="s2">"forwarder"</span><span class="p">][</span><span class="s2">"port"</span><span class="p">]</span> <span class="cp">%&gt;</span>
    'ssl_certificate' =&gt; '<span class="cp">&lt;%=</span> <span class="n">node</span><span class="p">[</span><span class="s2">"selfsigned_ssl"</span><span class="p">][</span><span class="s2">"directory"</span><span class="p">]</span> <span class="cp">%&gt;&lt;%=</span> <span class="n">node</span><span class="p">[</span><span class="s2">"selfsigned_ssl"</span><span class="p">][</span><span class="s2">"ssl_cert"</span><span class="p">]</span> <span class="cp">%&gt;</span>'
    'ssl_key' =&gt; '<span class="cp">&lt;%=</span> <span class="n">node</span><span class="p">[</span><span class="s2">"selfsigned_ssl"</span><span class="p">][</span><span class="s2">"directory"</span><span class="p">]</span> <span class="cp">%&gt;&lt;%=</span> <span class="n">node</span><span class="p">[</span><span class="s2">"selfsigned_ssl"</span><span class="p">][</span><span class="s2">"ssl_key"</span><span class="p">]</span> <span class="cp">%&gt;</span>'
  }
}

output {
  redis {
    'host' =&gt; '127.0.0.1'
    'data_type' =&gt; 'list'
    'key' =&gt; 'logstash'
  }
}
</code></pre></div>
<p>Unfortunately I ran out of time during our hackweek due to other issues I had to look at outside of this project to get the implementation down 100% but this is the rough idea for how it would look. I hope to make a follow up post when we have fully implemented the desired architecture. For now, this documents some of the learnings I gained while working on this project for a few days.</p>

</div>

<div>
  <a class="twitter-share-button" href="https://twitter.com/share?size=large">Tweet</a>
</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/devops/infrastructure/deploymenttools/2016/02/08/spinnaker-deployment-pipelines.html">
            Spinnaker deployment pipelines
            <small>08 Feb 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/devops/infrastructure/deploymenttools/2015/12/08/setup-okta-saml-with-spinnaker.html">
            Setting up Spinnaker authentication with Okta and SAML
            <small>08 Dec 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/devops/workingwithteams/continuousdelivery/infrastructure/2014/11/21/team-workflow-using-littlechef-to-manage-infrastructure.html">
            Team workflow using littlechef to manage infrastructure
            <small>21 Nov 2014</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>


      <!-- Footer -->
      <div class="footer">
      </div>

    </div>
  </body>
</html>
